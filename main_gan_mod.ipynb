{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"main_gan_mod.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1jDAVwVv4JtN17AarHjsOW74XafdCrXLi","authorship_tag":"ABX9TyNb5gInAdmZS/Dv3SbahLo7"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"iCGXNprcCNXE","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"b430b4ab-b3ec-4d2d-c2a8-5d4401fdd71c","executionInfo":{"status":"ok","timestamp":1589457666765,"user_tz":-540,"elapsed":13318,"user":{"displayName":"Rushikesh Handal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgOz3selWel07DhMWJ_Qtdhqke75MMSNUhZbZegnA=s64","userId":"09084884598721117870"}}},"source":["import os\n","import os\n","import shutil\n","import sys\n","\n","import numpy as np\n","from scipy import sparse\n","\n","import matplotlib.pyplot as plt\n","#%matplotlib inline\n","\n","import seaborn as sn\n","sn.set()\n","\n","import pandas as pd\n","\n","\n","\n","import bottleneck as bn\n","data_root = 'drive/My Drive/reco/data/'\n","save_dir = data_root+ 'lists'\n","pro_dir = save_dir\n","\n","# Model in pytorch\n","## l2 regularizers are included by default in optimizers??\n","##will defining the model tf way later\n","\n","#gen train-drop\n","#disc combination of drop and generated output pair\n","#and real and drop for real ones\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.parallel # not using multi-gpu for the time being\n","from torch.autograd import Variable\n","\n","class make_data(nn.Module):\n","    def __init__(self):\n","      super(make_data,self).__init__()\n","    def forward(self,inp,drop_prob=0):\n","      x1 = torch.nn.functional.normalize(inp,p=2,dim=1) #default 2nd degree normalizer\n","      x2 = torch.nn.Dropout(p=drop_prob)(x1)\n","      return x2\n","\n","class model_gen(nn.Module):\n","    def __init__(self,in_feature,latent_feature,weight_deca=0.0):\n","      super(model_gen, self).__init__()\n","      self.weight_decay = weight_deca\n","      self.linear1 = nn.Linear(in_feature, latent_feature, bias=True)\n","      self.linear2 = nn.Linear(latent_feature, in_feature, bias=True)\n","      self.tanh = nn.Tanh()\n","    def forward(self,inp,drop_prob=0):\n","      \n","      x_lat = self.tanh(self.linear1(inp))\n","\n","      x_out = self.linear2(x_lat)\n","\n","      return x_out\n","\n","    def loss(self,pred,gt):\n","      pred = nn.functional.log_softmax(pred,dim=-1)\n","      loss_mat = pred*gt\n","      loss = -loss_mat.mean(1)\n","      return loss.mean(0).view(1)\n","\n","class model_disc(nn.Module):\n","    def __init__(self,in_feature,latent_feature,weight_deca=0.0):\n","      super(model_disc, self).__init__()\n","      self.weight_decay = weight_deca\n","      self.linear1 = nn.Linear(in_feature, latent_feature, bias=False)\n","      self.relu = nn.ReLU()\n","      self.linear2 = nn.Linear(latent_feature, 1, bias=False)\n","      \n","    def forward(self,inp,drop_prob=0):\n","      \n","      x1 = self.relu((self.linear1(inp)))\n","\n","      x_out = self.linear2(x1)\n","      x_out = x_out.mean(0).view(1)\n","\n","      return x_out\n","\n","\n","unique_sid = list()\n","with open(os.path.join(pro_dir, 'unique_sid.txt'), 'r') as f:\n","    for line in f:\n","        unique_sid.append(line.strip())\n","\n","n_items = len(unique_sid)\n","\n","def load_train_data(csv_file):\n","    tp = pd.read_csv(csv_file)\n","    n_users = tp['uid'].max() + 1\n","\n","    rows, cols = tp['uid'], tp['sid']\n","    data = sparse.csr_matrix((np.ones_like(rows),\n","                             (rows, cols)), dtype='float64',\n","                             shape=(n_users, n_items))\n","    return data\n","\n","train_data = load_train_data(os.path.join(pro_dir, 'train.csv'))\n","\n","\n","def load_tr_te_data(csv_file_tr, csv_file_te):\n","    tp_tr = pd.read_csv(csv_file_tr)\n","    tp_te = pd.read_csv(csv_file_te)\n","\n","    start_idx = min(tp_tr['uid'].min(), tp_te['uid'].min())\n","    end_idx = max(tp_tr['uid'].max(), tp_te['uid'].max())\n","\n","    rows_tr, cols_tr = tp_tr['uid'] - start_idx, tp_tr['sid']\n","    rows_te, cols_te = tp_te['uid'] - start_idx, tp_te['sid']\n","\n","    data_tr = sparse.csr_matrix((np.ones_like(rows_tr),\n","                             (rows_tr, cols_tr)), dtype='float64', shape=(end_idx - start_idx + 1, n_items))\n","    data_te = sparse.csr_matrix((np.ones_like(rows_te),\n","                             (rows_te, cols_te)), dtype='float64', shape=(end_idx - start_idx + 1, n_items))\n","    return data_tr, data_te\n","\n","vad_data_tr, vad_data_te = load_tr_te_data(os.path.join(pro_dir, 'validation_tr.csv'),\n","                                           os.path.join(pro_dir, 'validation_te.csv'))\n","\n","\n","N = train_data.shape[0]\n","idxlist = np.arange(N)\n","\n","# training batch size\n","batch_size = 500\n","batches_per_epoch = int(np.ceil(float(N) / batch_size))\n","\n","N_vad = vad_data_tr.shape[0]\n","idxlist_vad = range(N_vad)\n","\n","# validation batch size (since the entire validation set might not fit into GPU memory)\n","batch_size_vad = 2000\n","\n","# the total number of gradient updates for annealing\n","total_anneal_steps = 200000\n","# largest annealing parameter\n","anneal_cap = 0.2\n","\n","N = train_data.shape[0]\n","idxlist = np.arange(N)\n","idxlist\n","\n","def load_tr_te_data(tp_tr, tp_te):\n","    #tp_tr = pd.read_csv(csv_file_tr)\n","    #tp_te = pd.read_csv(csv_file_te)\n","\n","\n","    start_idx = min(tp_tr['uid'].min(), tp_te['uid'].min())\n","    end_idx = max(tp_tr['uid'].max(), tp_te['uid'].max())\n","\n","    rows_tr, cols_tr = tp_tr['uid'] - start_idx, tp_tr['sid']\n","    rows_te, cols_te = tp_te['uid'] - start_idx, tp_te['sid']\n","\n","    data_tr = sparse.csr_matrix((np.ones_like(rows_tr),\n","                             (rows_tr, cols_tr)), dtype='float64', shape=(end_idx - start_idx + 1, num_item))\n","    data_te = sparse.csr_matrix((np.ones_like(rows_te),\n","                             (rows_te, cols_te)), dtype='float64', shape=(end_idx - start_idx + 1, num_item))\n","    return data_tr, data_te\n","\n","vad_data_tr = vad_data_tr\n","vad_data_te = vad_data_te\n","\n","import bottleneck as bn\n","def NDCG_binary_at_k_batch(X_pred, heldout_batch, k=100):\n","    '''\n","    normalized discounted cumulative gain@k for binary relevance\n","    ASSUMPTIONS: all the 0's in heldout_data indicate 0 relevance\n","    '''\n","    batch_users = X_pred.shape[0]\n","    idx_topk_part = bn.argpartition(-X_pred, k, axis=1)\n","    topk_part = X_pred[np.arange(batch_users)[:, np.newaxis],\n","                       idx_topk_part[:, :k]]\n","    idx_part = np.argsort(-topk_part, axis=1)\n","    # X_pred[np.arange(batch_users)[:, np.newaxis], idx_topk] is the sorted\n","    # topk predicted score\n","    idx_topk = idx_topk_part[np.arange(batch_users)[:, np.newaxis], idx_part]\n","    # build the discount template\n","    tp = 1. / np.log2(np.arange(2, k + 2))\n","\n","    DCG = (heldout_batch[np.arange(batch_users)[:, np.newaxis],\n","                         idx_topk].toarray() * tp).sum(axis=1)\n","    IDCG = np.array([(tp[:min(n, k)]).sum()\n","                     for n in heldout_batch.getnnz(axis=1)])\n","    return DCG / IDCG\n","\n","# weight initialization\n","def weights_init(m):\n","    classname = m.__class__.__name__\n","    if classname.find('Linear') != -1:\n","        torch.nn.init.xavier_uniform_(m.weight.data)\n","        torch.nn.init.normal_(m.bias.data,std=0.001)\n","        print (\"got here\")\n","        #m.bias.data.clamp_(-2*0.001, 2*0.001) # this will set the values to 0 but \n","        # in tf the values are redrawn : write a function for this if necessary\n","\n","#call model\n","num_item = len(unique_sid)\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","model_data = make_data()\n","model_data.apply(weights_init)\n","model_data.to(device)\n","\n","model_disc = model_disc(num_item*2,200,weight_deca=0.01 /500)\n","#model_disc.apply(weights_init)\n","model_disc.to(device)\n","\n","model_gen = model_gen(num_item,200,weight_deca=0.01 /500)\n","model_gen.apply(weights_init)\n","model_gen.to(device)\n","\n","optimizer_gen = torch.optim.Adam(model_gen.parameters(), lr=0.00005, weight_decay=0.0)\n","optimizer_disc = torch.optim.Adam(model_disc.parameters(), lr=0.00005, weight_decay=0.0)\n","\n","inputs = torch.FloatTensor(batch_size,num_item )\n","out_gr = torch.FloatTensor(batch_size,num_item )\n","\n","inputs.to(device)\n","out_gr.to(device)\n","\n","one = torch.FloatTensor([1])\n","mone = one * -1\n","one = one.to(device)\n","mone = mone.to(device)\n","\n","#for validation data\n","batch_size_vd = batch_size_vad\n","no_batches = 10000/batch_size_vd\n","batch_list_vd = np.arange((no_batches)).astype('int32')\n","rng = np.random.default_rng()\n","rng.shuffle(batch_list_vd)\n","\n","\n","\n"],"execution_count":2,"outputs":[{"output_type":"stream","text":["got here\n","got here\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Il0qjYFUDvS0","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"78a95830-e0b7-43ac-973c-7c7043dff12b","executionInfo":{"status":"ok","timestamp":1589459562256,"user_tz":-540,"elapsed":1822313,"user":{"displayName":"Rushikesh Handal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgOz3selWel07DhMWJ_Qtdhqke75MMSNUhZbZegnA=s64","userId":"09084884598721117870"}}},"source":["\n","# loop for training\n","\n","138494/500\n","batches_per_epoch\n","batch_list = np.arange((batches_per_epoch-2))\n","rng = np.random.default_rng()\n","#batch_list = np.array(batch_list)\n","#list(range(batch_list))\n","#print (batch_list)\n","rng.shuffle(batch_list)\n","\n","n_epochs = 200\n","len_epoch = N//batch_size\n","arr_disc= np.arange(len_epoch)\n","arr_disc = np.concatenate((arr_disc,arr_disc,arr_disc,arr_disc,arr_disc))\n","np.random.shuffle(arr_disc)\n","\n","arr_gen1 = np.arange(len_epoch)\n","arr_gen1 = np.concatenate((arr_gen1,arr_gen1,arr_gen1,arr_gen1,arr_gen1))\n","np.random.shuffle(arr_gen1)\n","\n","arr_gen2 = np.arange(len_epoch)\n","np.random.shuffle(arr_gen2)\n","len_epoch\n","\n","\n","#print (batch_list)\n","ndcg_vad =[]\n","from torch.autograd import Variable\n","ndcgs_vad=[]\n","\n","for epoch in range(n_epochs):\n","  rng.shuffle(arr_gen2)\n","  i = 0\n","\n","  while (i<len(arr_gen2)):\n","    #data = training_data[arr_gen2[i]*batch_size:(arr_gen2[i]+1)*batch_size].toarray()\n","    \n","    \n","    #real_cpu = data\n","\n","    model_gen.zero_grad()\n","    \n","    st_idx = arr_gen2[i]*batch_size\n","    end_idx = min(st_idx + batch_size, N)\n","    X = train_data[idxlist[st_idx:end_idx]]\n","    if sparse.isspmatrix(X):\n","      X = X.toarray()\n","    X = X.astype('float32')\n","    input_gen1=X\n","\n","    inputv=input_gen1\n","    inputv = Variable(torch.Tensor(inputv)).to(device)\n","    inputs = Variable(model_data(inputv,drop_prob=0.5).data)\n","    pred = model_gen(inputs)\n","    \n","    #print (inputv.shape)\n","    \n","    los = model_gen.loss(pred,inputv) # or input only??\n","    #los_reg = model.l2_reg().to(device)\n","    loss_total = los#+los_reg\n","    #print (los.shape)\n","    #print ('loss is {0}'.format(los.data[0]) )\n","    loss_total.backward()\n","    optimizer_gen.step() \n","    i+=1\n","  print ('epoch is ',epoch)\n","    #print (i,len(arr_gen2))\n","##Validate the pre-trained model"],"execution_count":3,"outputs":[{"output_type":"stream","text":["epoch is  0\n","epoch is  1\n","epoch is  2\n","epoch is  3\n","epoch is  4\n","epoch is  5\n","epoch is  6\n","epoch is  7\n","epoch is  8\n","epoch is  9\n","epoch is  10\n","epoch is  11\n","epoch is  12\n","epoch is  13\n","epoch is  14\n","epoch is  15\n","epoch is  16\n","epoch is  17\n","epoch is  18\n","epoch is  19\n","epoch is  20\n","epoch is  21\n","epoch is  22\n","epoch is  23\n","epoch is  24\n","epoch is  25\n","epoch is  26\n","epoch is  27\n","epoch is  28\n","epoch is  29\n","epoch is  30\n","epoch is  31\n","epoch is  32\n","epoch is  33\n","epoch is  34\n","epoch is  35\n","epoch is  36\n","epoch is  37\n","epoch is  38\n","epoch is  39\n","epoch is  40\n","epoch is  41\n","epoch is  42\n","epoch is  43\n","epoch is  44\n","epoch is  45\n","epoch is  46\n","epoch is  47\n","epoch is  48\n","epoch is  49\n","epoch is  50\n","epoch is  51\n","epoch is  52\n","epoch is  53\n","epoch is  54\n","epoch is  55\n","epoch is  56\n","epoch is  57\n","epoch is  58\n","epoch is  59\n","epoch is  60\n","epoch is  61\n","epoch is  62\n","epoch is  63\n","epoch is  64\n","epoch is  65\n","epoch is  66\n","epoch is  67\n","epoch is  68\n","epoch is  69\n","epoch is  70\n","epoch is  71\n","epoch is  72\n","epoch is  73\n","epoch is  74\n","epoch is  75\n","epoch is  76\n","epoch is  77\n","epoch is  78\n","epoch is  79\n","epoch is  80\n","epoch is  81\n","epoch is  82\n","epoch is  83\n","epoch is  84\n","epoch is  85\n","epoch is  86\n","epoch is  87\n","epoch is  88\n","epoch is  89\n","epoch is  90\n","epoch is  91\n","epoch is  92\n","epoch is  93\n","epoch is  94\n","epoch is  95\n","epoch is  96\n","epoch is  97\n","epoch is  98\n","epoch is  99\n","epoch is  100\n","epoch is  101\n","epoch is  102\n","epoch is  103\n","epoch is  104\n","epoch is  105\n","epoch is  106\n","epoch is  107\n","epoch is  108\n","epoch is  109\n","epoch is  110\n","epoch is  111\n","epoch is  112\n","epoch is  113\n","epoch is  114\n","epoch is  115\n","epoch is  116\n","epoch is  117\n","epoch is  118\n","epoch is  119\n","epoch is  120\n","epoch is  121\n","epoch is  122\n","epoch is  123\n","epoch is  124\n","epoch is  125\n","epoch is  126\n","epoch is  127\n","epoch is  128\n","epoch is  129\n","epoch is  130\n","epoch is  131\n","epoch is  132\n","epoch is  133\n","epoch is  134\n","epoch is  135\n","epoch is  136\n","epoch is  137\n","epoch is  138\n","epoch is  139\n","epoch is  140\n","epoch is  141\n","epoch is  142\n","epoch is  143\n","epoch is  144\n","epoch is  145\n","epoch is  146\n","epoch is  147\n","epoch is  148\n","epoch is  149\n","epoch is  150\n","epoch is  151\n","epoch is  152\n","epoch is  153\n","epoch is  154\n","epoch is  155\n","epoch is  156\n","epoch is  157\n","epoch is  158\n","epoch is  159\n","epoch is  160\n","epoch is  161\n","epoch is  162\n","epoch is  163\n","epoch is  164\n","epoch is  165\n","epoch is  166\n","epoch is  167\n","epoch is  168\n","epoch is  169\n","epoch is  170\n","epoch is  171\n","epoch is  172\n","epoch is  173\n","epoch is  174\n","epoch is  175\n","epoch is  176\n","epoch is  177\n","epoch is  178\n","epoch is  179\n","epoch is  180\n","epoch is  181\n","epoch is  182\n","epoch is  183\n","epoch is  184\n","epoch is  185\n","epoch is  186\n","epoch is  187\n","epoch is  188\n","epoch is  189\n","epoch is  190\n","epoch is  191\n","epoch is  192\n","epoch is  193\n","epoch is  194\n","epoch is  195\n","epoch is  196\n","epoch is  197\n","epoch is  198\n","epoch is  199\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"pSIsD5t6C0ph","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"11a530d7-e300-480a-c786-60cdddf5f08b","executionInfo":{"status":"ok","timestamp":1589459565374,"user_tz":-540,"elapsed":3147,"user":{"displayName":"Rushikesh Handal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgOz3selWel07DhMWJ_Qtdhqke75MMSNUhZbZegnA=s64","userId":"09084884598721117870"}}},"source":["ndcg_dist = []\n","ndcg_dist = []\n","for bnum, st_idx in enumerate(range(0, N_vad, batch_size_vad)):\n","  end_idx = min(st_idx + batch_size_vad, N_vad)\n","  X = vad_data_tr[idxlist_vad[st_idx:end_idx]]\n","\n","  if sparse.isspmatrix(X):\n","     X = X.toarray()\n","  X = X.astype('float32')\n","  #print (j)\n","  data_vd_tr = X\n","  data_vd_te = vad_data_te[idxlist_vad[st_idx:end_idx]]#.toarray()\n","  with torch.no_grad():\n","    data_vad = (model_data((Variable(torch.Tensor(data_vd_tr)).to(device))))\n","    pred_val = model_gen(data_vad)\n","    pred_val = pred_val.cpu().detach().numpy()\n","    pred_val[data_vd_tr.nonzero()] = -np.inf\n","    ndcg_dist.append(NDCG_binary_at_k_batch(pred_val, data_vd_te))\n","  \n","ndcg_dist = np.concatenate(ndcg_dist)\n","ndcg_ = ndcg_dist.mean()\n","print (ndcg_)\n","  "],"execution_count":4,"outputs":[{"output_type":"stream","text":["0.41659939016967257\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"2r6ax6uYCqWw","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"8ef1ada1-4597-417d-c839-ab5f85214648"},"source":["\n","\n","ndcgs_vad=[]\n","for epoch in range(n_epochs):\n","  #data_iter = iter(dataloader)\n","  np.random.shuffle(idxlist)\n","  \n","  i = 0\n"," \n","  while i < len_epoch:\n","    #print (i)\n","    j =0\n","    for p in model_disc.parameters():\n","      p.requires_grad = True\n","    \n","\n","    while j < 5 and i<len_epoch:\n","      \n","      for p in model_disc.parameters():\n","        p.data.clamp_(-0.0001, 0.0001)\n","      st_idx = arr_disc[i*5+j]*batch_size\n","      end_idx = min(st_idx + batch_size, N)\n","      X = train_data[idxlist[st_idx:end_idx]]\n","      if sparse.isspmatrix(X):\n","        X = X.toarray()\n","      X = X.astype('float32')\n","      \n","      model_disc.zero_grad()\n","    \n","      inputs=X\n","      input_state = Variable(torch.Tensor(inputs)).to(device)\n","      \n","      input_drop = model_data(input_state,drop_prob=0.5)\n","      input_disc = Variable(torch.cat((input_state,input_drop), dim=-1))\n","\n","      errD_real = model_disc(input_disc)\n","      #print (mone)\n","      #print (errD_real.shape)\n","      errD_real.backward(one)\n","\n","      # train_with generated data\n","\n","      st_idx = arr_gen1[i*5+j]*batch_size\n","      end_idx = min(st_idx + batch_size, N)\n","      X = train_data[idxlist[st_idx:end_idx]]\n","      if sparse.isspmatrix(X):\n","        X = X.toarray()\n","      X = X.astype('float32')\n","\n","      input_gen=X\n","      input_state_gen = Variable(torch.Tensor(input_gen)).to(device)\n","      input_drop_gen = model_data(input_state_gen,drop_prob=0.5)\n","      with torch.no_grad():\n","        out_gen = Variable(model_gen(input_drop_gen).data)\n","      input_disc_fake = Variable(torch.cat((out_gen,input_drop_gen), dim=-1))\n","      errD_fake = model_disc(input_disc_fake)\n","      errD_fake.backward(mone)\n","\n","      loss_disc = errD_real - errD_fake\n","      optimizer_disc.step()\n","\n","      j+=1\n","\n","    #train the generator\n","\n","    for p in model_disc.parameters():\n","      p.requires_grad = False\n","    model_gen.zero_grad()\n","\n","    st_idx = arr_gen2[i]*batch_size\n","    end_idx = min(st_idx + batch_size, N)\n","    X = train_data[idxlist[st_idx:end_idx]]\n","    if sparse.isspmatrix(X):\n","      X = X.toarray()\n","    X = X.astype('float32')\n","    input_gen1=X\n","    input_state_gen1 = Variable(torch.Tensor(input_gen1)).to(device)\n","    input_drop_gen1 = Variable(model_data(input_state_gen1,drop_prob=0.5).data)\n","    out_gen1 = model_gen(input_drop_gen1)\n","    input_disc_fake1 = (torch.cat((out_gen1,input_drop_gen1), dim=-1))\n","    errD_fake1 = model_disc(input_disc_fake1)\n","    errD_fake1.backward(one)\n","    loss_gen = errD_fake1\n","    optimizer_gen.step() \n","    i+=1\n","\n","  ndcg_dist = []\n","  ndcg_dist = []\n","  for bnum, st_idx in enumerate(range(0, N_vad, batch_size_vad)):\n","    end_idx = min(st_idx + batch_size_vad, N_vad)\n","    X = vad_data_tr[idxlist_vad[st_idx:end_idx]]\n","\n","    if sparse.isspmatrix(X):\n","       X = X.toarray()\n","    X = X.astype('float32')\n","    #print (j)\n","    data_vd_tr = X\n","    data_vd_te = vad_data_te[idxlist_vad[st_idx:end_idx]]#.toarray()\n","    with torch.no_grad():\n","      data_vad = (model_data((Variable(torch.Tensor(data_vd_tr)).to(device))))\n","      pred_val = model_gen(data_vad)\n","      pred_val = pred_val.cpu().detach().numpy()\n","      pred_val[data_vd_tr.nonzero()] = -np.inf\n","      ndcg_dist.append(NDCG_binary_at_k_batch(pred_val, data_vd_te))\n","    \n","  ndcg_dist = np.concatenate(ndcg_dist)\n","  ndcg_ = ndcg_dist.mean()\n","  print (ndcg_)\n","  ndcg_vad.append(ndcg_)\n","\n","\n","\n","\n","  \n","\n","\n","\n","\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["0.21512829663820926\n","0.19795500682493258\n","0.17994800249617898\n","0.17196202489108783\n","0.17235393361780152\n","0.1770943854273701\n","0.17058894776473962\n","0.16651322457128237\n","0.18461862535381549\n","0.17421849809272627\n","0.1816644407080325\n","0.1850185600695816\n","0.17851326269247164\n","0.18605079734858979\n","0.17930108447948312\n","0.18532796384985917\n","0.18980080255811324\n","0.18251876154542082\n","0.19645583320334145\n","0.1908177948022461\n","0.18291422456241088\n","0.19155298750315852\n","0.19147935576667174\n","0.19218830747927518\n","0.19719190594356534\n","0.19836212239497594\n","0.1969190062846483\n","0.19436481419995183\n","0.1907965249343756\n","0.19092972515196543\n","0.19593578713588194\n","0.2004572213619385\n","0.1982310620105809\n","0.19510306793221538\n","0.20178154294147546\n","0.20179007698894977\n","0.19528466056840638\n","0.20287046787954524\n","0.2011796566935348\n","0.1995538442190597\n","0.20262985631431987\n","0.20187586625887016\n","0.20284983579177746\n","0.19992835395799777\n","0.20702400187586315\n","0.20086979524971982\n","0.20560804334398464\n","0.2083329138438321\n","0.20263372415590383\n","0.20801370727818885\n","0.20556302968393997\n","0.20431422433202495\n","0.21164835196574758\n","0.20615388140322047\n","0.20686281116499253\n","0.21269971618713476\n","0.20866011010590346\n","0.2055466834962529\n","0.21035016916744975\n","0.21208599608261733\n","0.2050921960007292\n","0.2098033848521709\n","0.21279905382269984\n","0.21400571473768942\n","0.21057796716021626\n","0.20985749264093612\n","0.21211267248939944\n","0.2157371074352426\n","0.21838890642261313\n","0.2166099567408668\n","0.21482239249417406\n","0.21076932787717556\n","0.21613598242778034\n","0.21555275883564295\n","0.2170806185305729\n","0.2187669980236255\n","0.2196079841379289\n","0.21835841393365862\n","0.2173996488235833\n","0.2199456989332945\n","0.2182009381096565\n","0.21826647936828572\n","0.2183042738733828\n","0.21718110481232838\n","0.217285103983936\n","0.21629211520278582\n","0.2166808212331132\n","0.2171346301974273\n","0.21619034771939832\n","0.21843049927716324\n","0.21840946977117065\n","0.21892011069592668\n","0.21667506960791047\n","0.2190793330402677\n","0.2220050735772745\n","0.2242429807280047\n","0.2229037107309677\n","0.21952249331834847\n","0.2204567431150113\n","0.21973435296706298\n","0.21979457170985314\n","0.21869885294969058\n","0.22073975862791262\n","0.22104947030009697\n","0.222951177693378\n","0.2252946998129563\n","0.22366292741692745\n","0.22010571954925282\n","0.22129459052699518\n","0.2262033520118966\n","0.22154492065211143\n","0.22169069004665662\n","0.2209941968395852\n","0.22294462243572138\n","0.22455756539689692\n","0.22505096529099314\n","0.22277369064954974\n","0.22142401109302967\n","0.22505830991336342\n","0.22358512040325787\n","0.22231967836660851\n","0.2244596437991472\n","0.2224760155240369\n","0.22257671672705573\n","0.2249730812636582\n","0.22279516445223144\n","0.22298373439221705\n","0.22488719859941203\n","0.2190037332500411\n","0.2231532914000695\n","0.22606191725119892\n","0.2230711590791397\n","0.2216076147820379\n","0.22503882100548972\n","0.22209484573586247\n","0.22317267886599362\n","0.22499209645908455\n","0.22078481962105798\n","0.22573034944144582\n","0.2223713145076299\n","0.22477952768751017\n","0.22063245251903077\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"yCvN6VbVDXlX","colab_type":"code","colab":{}},"source":["    \n","  "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KGi2PnhdDSlf","colab_type":"code","colab":{}},"source":["\n","\n","\n","import matplotlib.pyplot as plt\n","\n","#%matplotlib inline\n","%matplotlib inline\n","x = np.arange(1,201,1)\n","plt.plot(x,ndcg_vad,'r-',label='new_approach')\n","\n","plt.ylabel(\"Validation NDCG@100\")\n","plt.xlabel(\"Epochs\")\n","plt.legend()\n","plt.savefig(save_dir+'/out_map')"],"execution_count":0,"outputs":[]}]}